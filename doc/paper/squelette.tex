%
% Fichier exemple pour MajecSTIC 2009
% -----------------------------------
% Par le comité de pilotage MajecSTIC
% majecstic-pilotage@irisa.fr
% 
% Vous pouvez éditer ce fichier pour composer votre article. Respectez la 
% langue française, pour vous aider ce document comporte des consignes 
% typographiques ainsi que des conseils pour la composition des figures et 
% des algorithmes.
%
%
%
%% interventions
%%  -eric 8 Avril 09 // qqes modifs de style pour le moment
%%
%%%%%% NE PAS MODIFIER
%
% gillemets a la francaise
\def\leftnote#1{\leavevmode\vadjust{\setbox1=\vtop{\hsize 20mm
  \parindent=0pt\small\baselineskip=9pt
  \rightskip=4mm plus 4mm#1}
  \hbox{\kern-2cm\smash{\box1}}}}
% encore quelques petits symboles particuliers
  \font\myl=manfnt
  \def\panneau{{\myl\char"7F}}
  \def\boxone{{\myl\char"1C}}
  \def\boxtwo{{\myl\char"1D}}
  \def\ortf{{\myl\char"1E}}
  \def\fleurone{{\myl\char"26}}
  \def\fleurtwo{{\myl\char"27}}
  \def\diams{{\myl\char"23}}
  \def\cible{{\myl\char"24}}
  \def\carre{{\myl\char"25}}
  \def\fleche{{\myl\char"79}}
  \def\panneaubis{{\myl\char"7E}}
\def\panneauinverse{{\myl\char"00}}

% Conserver ces commandes (Debut)
\documentclass[twoside,a4paper,10pt]{article}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage[frenchb]{babel}
\usepackage{majecstic2009,euler,palatino}
\usepackage[french,ruled,vlined,linesnumbered]{algorithm2e}
\dontprintsemicolon
\Setnlskip{0.5em}
\incmargin{1.2em}
\usepackage{epsfig,shadow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{cite}
\usepackage{vmargin}
\pagestyle{myheadings}
\setpapersize{A4}
% Conserver ces commandes (Fin)

%===========================================================
%                               Title
%===========================================================
\newcommand{\filet}{\noindent\rule[0mm]{\textwidth}{0.2mm}}

\toappear{1} % Conserver cette ligne pour la version finale

\begin{document}

\parindent=0pt

% À FAIRE modifier en ajoutant votre titre.
% Le titre courant peut-être trop long, dans ce cas indiquez un titre 
% plus court dans la commande shorttitle (sinon, indiquez le même)
\title{\Large\bf Interrogations de moteurs de recherche par des requêtes formulées en langage naturel}

% À FAIRE modifier en ajoutant dans le premier champ vos noms et dans votre
% titre. Attention, il faut que ca tienne sur une largeur de page, donc il 
% peut etre necessaire de donner un titre plus court, ou de ne mettre que les
% noms de famille des auteurs
% Exemples:
% \markboth{Josiane Balasko, Muriel Robin \& Yves Montand}{Nos années palace.}
% \markboth
% {Dupont, Dupond, Haddock, Milou, Tintin \& Tournesol}
% {De l'exploration des profondeurs à la conquète spatiale}
\markboth
{Ludovic Bonnefoy, Romain Deveaud \& Eric Charton}
{Interrogations de moteurs de recherche par des requêtes formulées en langage naturel}

% À FAIRE Indiquez vos noms ici. Nom1, Nom2 et Nom3. Utilisez $^i$ pour 
% l'adresse i.
\author{Ludovic Bonnefoy$^1$, Romain Deveaud$^1$ et Eric Charton$^2$}

% À FAIRE Indiquez vos adresses ici. Attention, la première adresse commence
% juste après le { de la commande.
\address{1: Centre d'Enseignement et de Recherche en Informatique / Université d'Avignon et des Pays de Vaucluse \texttt{ludovic.bonnefoy}, \texttt{romain.deveaud} \texttt{\{@etd.univ-avignon.fr\}}\\
2: LIA / Université d'Avignon et des Pays de Vaucluse, 339 chemin des Meinajaries, 84911 Avignon \texttt{eric.charton@univ-avignon.fr}
}

\maketitle

%===========================================================         %
%R\'esum\'e
%===========================================================  

\Resume{La difficulté de la tâche d'interprétation de requêtes en langue naturelle réside dans la transformation d'une phrase grammaticale en une requête pertinente pour interroger un système de RI ou d'extraction d'information. Dans cet article, nous présentons un système d'extraction d'information associé à un analyseur de requêtes, dédié à l'analyse et à l'interprétation de questions factuelles formulées en langue naturelle issues de la campagne TREC. Notre système fournit des réponses en exploitant un système composé d'une ressource ontologique et sémantique issue d'un corpus encyclopédique, dont on aura extrait les réponses candidates à l'aide de méthodes statistiques.}
%% Deja dans la fin de la section 1
%Nous obtenons des performances du niveau de l'état de l'art.

%Hors l'étape d à dessin de reformuler une interrogation qui soit efficace sur un système de RI. 
%Malgré le fait que ce domaine commence à être couvert par quelques grands noms de l'informatique, les réalisations disponibles à ce jour ne peuvent être considérées que comme des prototypes compte tenu des résultats actuellement visibles. Ceci peut-être expliqué par la grande diversité du langage naturel et par les nombreux sens qui peuvent être donnés à un même mot ou une même expression. Les différents critères sémantiques naturellement présents dans les phrases permettent notamment de combiner la recherche d'information classique - extrayant des documents comparés à des termes, ou mots-clés - avec des \og{}entités\fg{} ou des \og{}concepts\fg{} pouvant être apparentés à des catégories sémantiques (par exemple : \og{}personne\fg{},\og{}organisation\fg{},\og{}date\fg{}...). Nous proposerons dans cet article un système de catégorisation et d'extraction de mots-clés à partir de phrases formulées en langage naturel.}
% Une catégorisation plus fine peut également être réalisée, permettant ainsi d'obtenir une meilleure granularité des résultats ; une base de données catégorisée doit néanmoins être disponible pour pouvoir appliquer ce type de recherche d'information.}

\Abstract{Nowadays, requesting a search engine with natural language requests is a significant issue in the information retrieval research field, and some of its biggest actors begin to take it seriously. Some prototypes are actually available, but the error rate, inferred by the huge diversity of natural language and the different semantics of words or expressions, is still too large. Sentences naturally contain semantic criteria such as "entities", "concepts" or "categories" which can be combined with standard information retrieval in order to filter the documents with these semantic categories (e.g. "person", "organisation", "date"...). In this article we propose a categorization and keywords extraction system for natural language sentences.}

\MotsCles{Sémantique, catégorisation, langage naturel, recherche d'information.}
\Keywords{Information retrieval, semantic, categorization, natural language.}
%=========================================================
\section{Introduction}
%=========================================================
%%
%% De manière générale soyez plus directs, évitez les circonvolutions litéraires de type ``De nos Jours'' etc
%%

%\par De nos jours, les moteurs de recherche sont un outil pleinement utilisable par les personnes averties et habituées, malgré la volonté d'en améliorer l'accessibilité. En effet, le processus consistant à passer d'une interrogation à un enchainement de mots-clés pertinents retranscrivant correctement la pensée initiale n'est pas un exercice aisé, du moins pour les personnes qui ne sont pas familières avec l'informatique ou internet, parmi lesquelles nous pouvons par exemple compter les enfants ou certaines personnes âgées.
%\par Il serait en effet idéal de pouvoir simplement formuler une question à un moteur de recherche et que celui-ci puisse donner la réponse ou du moins un ensemble de documents dans lesquels une ou des réponses se trouveraient. 
\par L'interprétation de requêtes écrites en langage naturel par un système de recherche d'information (RI) ou de dialogue est un enjeu important de l'ingénierie documentaire. Outre le fait que la requête en language naturelle rend possible la formulation d'une requête de recherche selon une forme bien plus précise que la simple saisie de mots clés, elle permet, dans le cadre de la compréhension des languages parlés et des systèmes de dialogue, de déployer des composants de compréhension (SLU) conviviaux pour les utilisateurs. 

\par Dans le cadre des systèmes de Question et Réponses (SQR), l'analyse d'une requête en language naturelle doit permettre de transformer en processus d'extraction d'information, une question factuelle ou complexe. La difficulté de la tâche d'interprétation de requêtes en langue naturelle réside dans la transformation d'une phrase grammaticale en une requête pertinente pour interroger un système de RI ou d'extraction d'information. 

\par Dans cet article, nous présentons le système \textit{NLGbAse QR}. Ce système est un extracteur d'information contenues dans une ontologie, associé à un analyseur de requêtes formulées en langue naturelle. Ce système est testé à l'analyse et à l'interprétation de questions factuelles formulées en langue naturelle issues de la campagne TREC. 
%% Deja dans l'intro %%
%Notre système fournit des réponses en exploitant un système composé d'une ressource ontologique et sémantique issue d'un corpus encyclopédique, dont on aura extrait les réponses candidates à l'aide de méthodes statistiques. 

\par La structure de cet article est la suivante: 

\par Nous décrivons dans notre introduction les grands principes de la littérature sur l'analyse de requêtes en langue naturelle, et son déploiement dans le cadre de systèmes de Question Réponses (SQR). Puis nous présentons les principaux systèmes de QR existants, et les méthodes de transformation de requêtes sémantiques et d'interrogation qu'ils mettent en oeuvre. Nous expliquons également dans cette section la relation existant entre ontologies issues de corpus encyclopédiques, et le rôle que ces ontologies peuvent jouer dans des SQR. 
\par Dans la section suivante, nous présentons l'architecture du SQR \textit{NLGbAse.QR}. Ce dernier est divisé en trois parties: analyse des requêtes en langue naturelle, transformation des requêtes en paramètres d'extraction, extracteur d'informations appliqué sur une ontologie. Puis nous détaillons dans la section \ref{details} ces trois modules et les algorithmes qu'ils sous-tendent. 
\par Dans la section \ref{expériences}, nous présentons une évaluation de notre système avec le corpus de question de TREC. Nous concluons par une analyse critique des résultats obtenus, qui sont du niveau de l'état de l'art, et introduisons nos perspectives futures de recherche. 

\section{Principe de l'analyse de requêtes en langue naturelle et des SQR}
%%% cette partie est intéressante pour l'état de l'art
\par Des sociétés telles que Google\footnote{http://www.google.com}, Powerset\footnote{http://www.powerset.com} (propriété de Microsoft) ou Hakia\footnote{http://www.hakia.com} sont actuellement fortement investies dans le développement de solutions sémantiques à l'interrogation des moteurs de recherche et l'enrichissement communautaire en est un élément central, au moins pour les deux premiers protagonistes. Ils ont en effet recours à de nombreux sites dont les informations sont éditées par des internautes contributeurs, Wikipédia\footnote{http://www.wikipedia.org} étant le plus célèbre d'entre eux.

\par C'est également notre cas, puisque le système que nous proposons s'interface avec NLGbAse\footnote{http://www.nlgbase.org}, une base de données classifiées provenant de Wikipédia qui peut être interrogée par le biais de trois moteurs de recherche différents. Le premier d'entre eux met en {\oe}uvre un algorithme calculant la \emph{similarité cosinus} entre l'ensemble des mots-clés entrés et les documents issus de Wikipédia ; le deuxième est semblable au premier en tous points, à l'exception que l'on peut affiner la recherche en précisant une catégorie, ainsi seuls les documents classifiés comme appartenant à la catégorie spécifiée seront relevés. Le troisième applique quant à lui un algorithme de compacité\footnote{Référence nécessaire} permettant de trouver une entité précise appartenant à une catégorie donnée, proche d'un ou plusieurs mots donnés, dans le document Wikipédia se rapportant à une entité nommée donnée, ce qui permet notamment de pouvoir proposer une réponse factuelle à une requête.

\par Ces outils constituent le système de recherche d'information sur lequel nous appliquons les sorties de notre propre système ; ce dernier peut, à partir d'une phrase en langage naturel - de préférence une question, donner les différents mots-clés et catégories attendus par les moteurs de recherche de NLGbAse, et ainsi obtenir une liste de résultats - et éventuellement des réponses factuelles - pertinents.
%Voulez-vous \og{}\emph{vraiment}\fg{} citer~\cite{bogdanoff:these,sonia:point} ?

\subsection{Encyclopédies, ontologies et SQR}

%%% description des ontologies et papiers de référence / WWW / dbpedia / Yago

%% \section{Architecture du système NLGbAse.QR}
%% Que faire dans cette section? A part expliquer pourquoi nous avons utilisé Link-Grammar et Wordnet et comment ils marchent, je ne vois pas.

\section{Algorithmes déployés}\label{details}
\subsection{Analyse morpho-syntaxique et couplage des mots}
\par Pour travailler sur la sémantique, il est indispensable de posséder des outils permettant à la machine de décomposer et d'analyser la structure des phrases. C'est pourquoi nous avons utilisé un analyseur morpho-syntaxique réalisant des couplages de mots selon leur position grammaticale dans la phrase et les liant selon leurs interdépendances~\cite{linkgrammar:paper}.
% A faire
%Ici une explication du travail effectué par le parser et notre méthode pour trouver les noms propres, l'objet etc... dans une phrase.

%=============================================================
\subsection{Catégorisation des phrases}
%=============================================================
\par Par définition, la sémantique crée naturellement différentes classes de sens plus ou moins générales, au niveau du mot comme de la phrase. Nous adhérons à l'idée selon laquelle la sémantique globale d'une phrase est déterminée par un nombre réduit de mots appartenant à une catégorie grammaticale précise, et c'est selon ce point de vue que nous allons présenter la catégorisation de phrases en catégories sémantiques larges que nous avons implémentée.
%=============================================================
\subsubsection{Catégorisation à base de règles simples}
%=============================================================

\par Notre approche pour catégoriser les questions fonctionne avec un ensemble de règles. Cet ensemble est relativement restreint car nous avons pu remarquer qu'une dizaine de règles environ pouvaient couvrir une majorité des cas, et qu'ensuite chaque petit gain se traduisait par la production d'un nombre croissant exponentiellement de nouvelles règles. Les règles que nous avons formulées se basent principalement sur les pronoms interrogatifs des questions. En voici la liste : 
\begin{itemize}
\item Who, Whom, Whose : \emph{pers} (Person).
\item Where, Whence, Wither : si il s'agit de trouver une catégorie pour le deuxième moteur de NLGbAse, et si un nom propre ou un objet est trouvé, la catégorie sera celle du \emph{nom propre} ou de l'\emph{objet} grammatical de la phrase (par exemple : \og{}Where did Patrick Sébastien study?\fg{}, il y a peu de chance de trouver dans la fiche de ce lieu une mention de cette personnalité et il est à priori plus judicieux de proposer la fiche de \emph{Patrick Sébastien}, dans laquelle l'utilisateur sera à même de touver l'information). Dans le cas contraire (ou si nous voulons une catégorie pour le troisième moteur), la catégorie \emph{loc} (Location) est attribuée.
\item How : nous appliquons la même procédure que précédemment, à l'exception près que si les premières conditions ne sont pas remplies, la catégorie \emph{unk} (Unknown) est attribuée. Pour ce pronom là nous avons ajouté quelques précisions lorsque nous cherchons une catégorie pour le troisème moteur : si le mot suivant directement \emph{how} fait partie de la liste suivante (far, few, great, little, many, much, tall, wide, high, big, old), la catégorie \emph{amount} est attribuée.
\item What, Why, Which : le principe est toujours le même, avec \emph{unk} (Unknown) pour valeur par défaut. Nous avons également établi, comme précédemment, une liste de mots pouvant être acceptés comme suivant directement le pronom interrogatif (comme par exemple day : \og{}What day is the Independance Day?\fg{}) et qui vont impliquer automatiquement l'attribution d'une catégorie (\emph{date} dans l'exemple précédent).
\end{itemize}
Nous allons maintenant détailler les méthodes de catégorisation des noms propres et des noms communs que nous avons mises en place.


%=============================================================
\subsubsection{Catégorisation par les noms propres utilisant NLGbAse}
%=============================================================
\par Comme nous l'avons vu, la majorité des règles ne nous permettent pas de trancher directement, nous devons donc compléter notre analyse par un autre moyen et cela passe notamment par la catégorisation des noms propres. Nous avons remarqué que dans la majorité des cas, si un nom propre est présent dans une question, il en est l'objet ou du moins l'objet est l'une de ses caractéristiques. Prennons par exemple ces deux questions \og{}What is the date of birth of Bruce Dickinson?\fg{}, \og{}Who is Batman's team-mate?\fg{} ; nous voyons bien que les informations désirées sont \emph{forcément} en relation avec nos noms propres.
\par Nous avons donc prit le parti de prendre comme catégorie la catégorie du nom propre se trouvant dans la question - s'il y en a un. Pour cela nous adressons une requête à un script issu de NLGbAse, qui comme nous l'avons vu associe une catégorie à chaque entité de Wikipédia, qui récupère la catégorie de l'entité correspondant à ce nom propre. Si une entité porte exactement le même nom alors la catégorie sera celle de cette entité ; si ce n'est pas le cas mais que des entités ont un nom similaire, alors la catégorie sera celle de la plus pertinente d'entres elles. Enfin si ce n'est pas le cas nous effectuons une recherche par \emph{TF.Idf}
%La solution la plus évidente aurait été de prendre pour catégorie celle du document ayant le meilleur score.
%Nous avons essayé une autre approche. 
en prenant la catégorie qui a le plus fort score, pour cela chaque catégorie se voit attribuée comme score la somme des scores des documents ayant cette catégorie. De ce fait la catégorie qui rassemble le plus de pertinence sera sélectionnée.
\par Cependant nous sommes conscients qu'il arrive parfois que cette stratégie ne soit pas idéale, comme pour : \og{}What is the name of Batman's car?\fg{}. Il y a des chances que cette méthode donne \emph{pers} (Person) comme catégorie attendue alors que la solution idéale aurait probablement été \emph{prod} (Product). Elle n'est néanmoins pas totalement inappropriée car nous devrions trouver l'information désirée dans la fiche Wikipédia de Batman, néanmoins l'accès à la réponse est moins direct.

%=============================================================
\subsubsection{Catégorisation par les noms communs utilisant WordNet}
%=============================================================

\par Toutes les questions ne comportent évidemment pas de noms propres mais généralement des noms communs, c'est pourquoi nous avons du trouver un moyen de traiter ces questions par une approche assez simple. La décomposition morpho-syntaxique nous permet généralement de trouver l'objet de la question, qui possède généralement une forte valeur sémantique ; c'est donc celui-ci que nous allons étudier. En effet, pour la question \og{}What are the generals?\fg{}, l'objet de la question est \og{}generals\fg{} ; l'enjeu est d'arriver à associer \og{}generals\fg{} à l'étiquette \emph{fonc.mil} (fonction militaire).
\par Pour arriver à cela nous utilisons WordNet\footnote{http://wordnet.princeton.edu} et ses hyperonymes ainsi que sa capacité à fournir une classe pour chaque mot : en effet WordNet associe déjà à la totalité des termes une étiquette, par exemple à \og{}general\fg{} WordNet associe \emph{noun.person}. L'étiquette que fournit WordNet est bien souvent satisfaisante, cependant son jeu d'étiquette ne correspond pas aux exigences d'Ester auquel notre projet doit se plier.
\par La première étape fut d'associer \og{}à la main\fg{} des étiquettes à des mots qui prendront le dessus sur celles de WordNet ; pour reprendre notre exemple, nous ne voulons pas l'étiquette \emph{pers} (Person) pour \og{}general\fg{} mais bien \emph{fonc.mil}. Cependant faire ce travail sur tous les mots demanderait un investissement titanesque, et nous avons pu palier à ce problème en réfléchissant aux mots les plus généraux possibles pour chaque catégorie dont nous avions besoin. Ensuite nous vérifions que les hyponymes de ces mots sur WordNet correspondaient bien à la même catégorie, et si ce n'était pas le cas nous sélectionnions tous les hyponymes pour lesquels c'est le cas et répétions cette opération. Nous sommes donc arrivés à une liste de mots caractérisant parfaitement chaque catégorie - et étant compatibles avec WordNet.
\par L'algorithme de catégorisation en lui-même consiste en une fonction récursive qui va vérifier si le nom commun ne fait pas partie des mots étiquetés. Si ce n'est pas le cas cette vérification est faite pour son hyperonyme, et ainsi de suite. La récursivité s'arrête si un mot associé à une étiquette est trouvé ou si on arrive sur l'hyperonyme de plus haut niveau. Dans le premier cas le mot de départ se voit associé cette étiquette et donc la catégorie recherchée aussi, dans le deuxième cas c'est l'étiquette associée par WordNet au mot de départ qui prévaut et qui est donc définie comme la catégorie recherchée.
\par Cette méthode utilisée seule ne peut bien évidemment pas couvrir tous les cas, néanmoins c'est l'association des différentes - mais surtout complémentaires - méthodes de catégorisation qui permet d'obtenir des résultats satisfaisants.

%Au final même si il est évident que nous ne couvrons pas tout les cas en seulement trois heures il est possible de couvrir un très grand nombre de cas puisque pour chaque mot étiqueté tout ses hyponymes reçoivent cette même étiquette.

%=============================================================
\subsection{Extraction des mots-clés}
%=============================================================
%Comme énoncé dans la section précédente nous utilisons un parser pour arriver à repérer les termes importants pour la catégorisation et il en est de même pour l'extraction des mots clés.

\par Comme nous l'avons expliqué, les différents moteurs de recherche de NLGbAse n'attendent pas les mêmes entrées, nous allons donc détailler ici les deux types d'extraction de mots-clés - ou mots pertinents.

\subsubsection{Extraction destinées aux moteurs de recherche d'information par similarité cosinus}
\par Dans un premier temps, les mots-outils de la phrase sont automatiquement supprimés à l'aide d'un anti-dictionnaire. Nous utilisons ensuite l'analyse morpho-syntaxique de la phrase, et notamment l'arbre constitutif, pour récupérer les mots - qui ne sont pas des mots-outils - qui constituent groupes nominaux ; malgrès son aspect simple, voire simpliste, nous avons pu prouver empyriquement son efficacité.

\subsubsection{Extraction destinées au moteur de recherche d'information par algorithme de compacité (question-réponse)}
%Pour celui ci la démarche est assez proche mais le besoin de données est différent.
\par Nous l'avons déjà précisé plus haut, ce troisième moteur accepte plusieurs entrées différentes, et notamment deux champs de mots-clés. Le premier champ est une entité nommée qui va déterminer dans quel document sera appliqué l'algorithme de compacité, tandis que le deuxième champ consiste en une liste de mots qui représentent l'information cherchée ; par exemple pour la question \og{}When was Albert Einstein born?\fg{}, le mot \og{}born\fg{} devrait être sélectionné car l'information recherchée - une date de naissance en l'occurence - se trouvera certainement très proche de ce mot.
\par Dans un premier temps nous devons donc trouver l'entité nommée ; si un nom propre est présent dans la question, il sera directement utilisé comme entité nommée. Dans le cas contraire, nous éxécutons une requête sur NLGbAse avec l'objet de la question afin de récupérer le nom de l'entité nommée la plus pertinente. 
\par Dans un second temps vient l'extraction des mots porteurs de sens ; il s'agit tout d'abord de supprimer tous les mots-outils, les noms propres et le verbe présents dans la phrase. Il s'agit ensuite de récupérer les synonymes des mots restant avec WordNet ; pour reprendre notre exemple précédent, nous ne savons pas si le mot \og{}born\fg{} sera effectivement employé dans le document dans lequel l'information sera cherchée, c'est pourquoi nous cherchons des dérivations afin de les rajouter à notre liste et ainsi améliorer nos chances de trouver l'information. Toujours pour notre exemple, cette recherche de synonymes pourrait nous mener au mot \og{}birth\fg{}, qui serait en effet intéressant à garder dans l'optique de la recherche d'une date de naissance.


%==============================================================
\section{Expériences et résultats}\label{expériences}
%==============================================================

%%% modif eric
\par La vocation de notre algorithme est de transformer une question en langue naturelle en une requête compatible avec un système de RI.  Pour mesurer les performances de notre système, nous évaluons dans un premier temps sa capacité à étiqueter une question en vue de l'extraction des informations requise pour construire une requête. Dans un second temps nous construisons une requête d'après les informations extraites et mesurons la pertinence des résultats retournés par l'un des trois moteurs de NLGbAse.

\subsection{Corpus de référence et standard de mesure}

\par Nous avons utilisé pour ces mesures des corpus de questions reliées à une réponse factuelles ou de type liste. Des corpus de références ont été mis au points dans le cadre de campagnes d'évaluation telles que Trec 8 %%% citer %%% 
\par Le corpus \textit{Question Answering Collections} de Trec est composé de XXX questions factuelles et des réponses standard qui doivent être retournées\footnote{En téléchargement sur http:\/\/trec\.nist\.gov\/data\/qa\/t2004\_qadata.html}. Ces paires de questions et réponses sont organisées comme suit. Un fichier XML décrit les question factuelles: 

\begin{verbatim}
<target id="2" text="Fred Durst">
<qa><q id="2.1" type="FACTOID">
What is the name of Durst's group?
</q></qa>
<qa>
<q id="2.2" type="FACTOID">
What record company is he with?
</q>
</qa>
[...]
\end{verbatim}

Des réponses \og{}standard\fg{} sont livrées sous forme de liste : 

\begin{verbatim}
2.5 1	vital	tattoo artist
2.5 2	vital	operated skate park
\end{verbatim}

\par Ce corpus n'étant pas initiallement prévu pour vérifier l'expérience d'étiquetage de questions tel que prévu dans le cadre de notre algorithme, nous avons procédé à son enrichissement.
\par Cet enrichissement consiste a attribuer à chaque question la catégorie sémantique de la réponse attendue (nous n'avons retenu que les classes racines à savoir \emph{pers}, \emph{org}, \emph{loc}, \emph{date}, \emph{amount} et \emph{unk}), ainsi que les différents mots-clés qui peuvent permettre d'obtenir une réponse. Nous disposons ainsi d'un corpus de questions et de réponses de références complétées par des informations sémantiques compatibles avec notre système\footnote{Ce corpus complété est disponible sur www.nlgbase.org}. 
\par Ainsi, dans l'exemple précité, nous complétons la description de la question par plusieurs étiquettes de classes, à savoir que l'entité source est une personne, et que la réponse cible est un nom d'organisation (celui du groupe musical), et d'éditeur de disque: 

\begin{verbatim}
<target id="2" text="Fred Durst">
<k>PERS</k>
<qa><q id="2.1" type="FACTOID">
What is the name of Durst's group?
</q><kt>ORG</kt>
</qa>
<qa>
<q id="2.2" type="FACTOID">
What record company is he with?
</q><kt>ORG</kt>
</qa>
[...]
\end{verbatim}

\par Le corpus est également composé de 108 questions prélevées aléatoirement sur le site AnswerBus\footnote{http://www.answerbus.com/}, qui est également un système de rechercher d'information par question-réponse. La particularité de ces questions est qu'elles ont été formulées par des utilisateurs, une majorité d'entre-elles n'est donc pas grammaticalement parfaite ; nous avons trouvé ce corpus intéressant pour tester notre système car cela nous permet de nous rapprocher de la réalité par rapport aux questions classiques de campagnes d'évaluation.

\par Nous avons lancé notre système et mesuré pour chaque question la validité de son étiquetage sémantique et de l'identification des mots clés pertinents pour une extraction de réponse, puis la pertinence des réponses retournées par le moteur de recherche par rapport à la réponse de référence. Ces mesures sont réalisées par un calcul de score de précision et le rappel, complété par le F-Score\footnote{Mesure harmonique combinant la précision et le rappel}.

%% retrait eric
%Les sorties réelles de notre système ont donc été comparées avec notre corpus, et nous avons ainsi pu calculer les diverses mesures présentées ci-dessous.

%\par Nous avons procédé à un certain nombre de tests et d'expériences afin d'évaluer les performances de la catégorisation comme de l'extraction des mots-clés. Nous n'avons retenu que les classes racines à savoir \emph{pers}, \emph{org}, \emph{loc}, \emph{date}, \emph{amount} et \emph{unk}. 

%\par Nous nous sommes appuyés sur un corpus de 
%% il faudra mettre le nombre de questions (107 pour le moment)
%questions issues de TREC 8 que nous avons étiquettées \og{}à la main\fg{} comme si c'était des sorties de notre système. Pour chaque question, nous avons déterminé la catégorie sémantique de l'information attendue et les différents mots-clés nécessaires pour une recherche correcte de cette information - comme nous l'avons déjà expliqué dans cet article. Les sorties réelles de notre système ont donc été comparées avec notre corpus, et nous avons ainsi pu calculer les diverses mesures présentées ci-dessous.

%% pas bon ça il faut être affirmatif et plus précis %%%
%les standards d'Ester 2 n'étant pas encore complètement implémentés sur la version anglaise de NLGbAse. Nous avons récupéré un corpus de questions formulées par des utilisateurs et nous les avons étiquetées en faisant mentalement le même travail que notre système, afin de pouvoir comparer ses résultats aux notres. 

\subsection{Mesures de la catégorisation sémantique}

\par Les résultats de l'attributions de catégories aux XXX phrases du corpus étiqueté sont présentés dans le tableau 1. 
%Nous avons calculé pour chacune d'elles la précision et le rappel, puis le F-Score\footnote{Mesure harmonique combinant la précision et le rappel} en découlant.
\par Ces résultats mettent en perspective le fait que des règles simples couplées à une catégorisation par recherche dans une base de données sémantique peuvent être viables. Les résultats de la catégorie \emph{org} peuvent être expliqués par le fait que nous n'avons pas pu définir de règle spécifique à cette catégorie sans impacter les résultats des autres catégories, notamment \emph{pers} et \emph{loc}.

\begin{table}[h]
    \begin{center}
        \begin{tabular}{|p{2.5cm}|l|l|l|}
            \hline
            Catégorie & (\={p}) & (\={r}) & (\={F}-s) \\
            \hline
            Pers & 0.89 & 0.92 & \textbf{0.90} \\
            \hline
            Org & 0.75 & 0.40 & \textbf{0.52} \\
            \hline
            Loc & 0.59 & 0.81 & \textbf{0.68} \\
            \hline
            Date & 0.89 & 0.89 & \textbf{0.89} \\
            \hline
            Amount & 0.78 & 0.63 & \textbf{0.70} \\
            \hline
            Unk & 0.70 & 0.65 & \textbf{0.67} \\
            \hline
            \hline
            Total & 0.77 & 0.72 & \textbf{0.73} \\
            \hline
        \end{tabular}
        \caption{\label{tab:results}Précision (\={p}), Rappel (\={r}), F-Score (\={F}-s) obtenus sur le corpus de test}
    \end{center}
\end{table}

\subsection{Mesures de l'extraction de mots-clés}

\par Nous avons également mesuré la pertinence des mots-clés extraits par notre système. Comme précédemment nous avons tout d'abord effectué \og{}à la main\fg{} le travail d'extraction, puis nous avons comparé ceci avec les résultats du système afin d'obtenir un pourcentage de satisfaction calculé selon la formule suivante : $$S(C) = \frac{\sum_{i = 1}^{N} P(Q_i)}{N}$$
\par Ils sont présentés dans les tableaux ci-dessous.

\begin{table}[htbp]
    \begin{center}
        \begin{tabular}{|p{12cm}|l|}
            \hline
            Type de mots-clés & (S(C)) \\
            \hline
            Mots-clés extraits pour une recherche par similarité cosinus & \textbf{70.83\%} \\
            \hline
            Entités nommées extraites pour une recherche de type question-réponse (compacité) & \textbf{60.78\%} \\
            \hline
            Mots-clés extraits pour une recherche de type question-réponse (compacité) & \textbf{76.47\%} \\
            \hline
        \end{tabular}
        \caption{\label{tab:results}Satisfaction (S(C)) obtenue sur le corpus de test}
    \end{center}
\end{table}

\par La grande majorité des erreurs dans la détection de mots-clés est due à une formulation grammaticale approximative de la question, ce qui induit en erreur l'analyseur morpho-syntaxique qui ne va pas correctement étiqueter certains mots.

%==============================================================
\section{Conclusion}
%==============================================================
Nous avons présenté un système analysant et décomposant des requêtes formulées en langage naturel dans le but d'en extraire leur catégorie sémantique ainsi que les mots porteurs de sens et d'information, destiné à être couplé avec un système de recherche d'information que nous avons présenté.
\bibliographystyle{majecstic}
\bibliography{references}

% À FAIRE: remerciements (ou supprimez les deux lignes suivantes)
%{\filet \small\\
%{\em Vous pouvez placer ici des remerciements}\\ \filet } 


\end{document}
% fin

